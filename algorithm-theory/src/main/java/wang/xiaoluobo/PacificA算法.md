# Microsoft PacificA算法([参考文章](http://www.voidcn.com/article/p-tmvvnrvw-bca.html))
PacificA是微软为大规模分布式存储系统开发的一个通用复制框架，也可称之为一个原型系统。该框架简单，实用，提供强一致性，并且可以适配不同的复制策略。它对于我们理解分布式系统的强一致性，容错，仲裁有很好的指导意义。

## 一、算法特点
1. 通用和抽象的复制框架，模型容易验证正确性，实现不同的策略实例  
2. 配置管理和数据复制分离，Paxos负责管理配置，主副本策略负责数据复制  
3. 将错误检测和配置更新内容在数据复制的交互里面，去中心化，降低瓶颈  

## 二、系统框架
1. 存储集群  
负责系统数据的读取和更新，通过使用多副本方式保证数据可靠性和可用性  
数据被存储在数据分片上，数据分片实际上是数据集合，每个数据分片的大小基本固定，
在存储节点上，数据分片表现为磁盘上的一个大文件。而数据分片也是多副本的基本单位。

2. 配置管理集群  
维护副本信息，包括副本的参与节点，主副本节点，当前副本的版本等  
配置管理集群维护的其实是分片的副本信息(分片位于哪些节点)。数据分片的副本称之为复制组(Replication Group)。
因为每个存储节点上会存在众多的数据分片，因此，每个存储节点会位于多个复制组中。

## 三、[PacificA算法原文](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.188.450&rep=rep1&type=pdf)
### **抽象**
****
大规模分布式存储系统已经普及，用于存储和处理不断增加的数据量。复制机制通常是在这种系统中实现高可用性和高吞吐量的关键。
对共识等基本问题的研究为复制协议奠定了坚实的基础。然而，实际复制机制的架构设计和工程问题仍然是一门艺术。

本文介绍了我们在为常用的基于日志的存储系统设计和实现复制方面的经验。我们提倡一种简单，实用且高度一致的通用复制框架。
我们表明该框架足够灵活，可以适应我们探索的各种不同的设计选择。使用名为PacificA的原型系统，我们实现了三种不同的复制策略，
所有这些都使用相同的复制框架。本文报告了详细的性能评估结果，尤其是失败，协调和恢复期间的系统行为。
****

### 1.引言
对越来越需要大规模存储系统来保存不断增加的数字化信息量。为了具有成本效益，这种系统通常用廉价的商品组件构建。
因此，故障成为一种规范，诸如复制之类的容错机制成为这种系统的可靠性和可用性的关键。

存在众所周知且可证明正确的复制协议(例如，Paxos [18])。但是，构建实用的分布式存储系统不仅仅是应用已知的复制协议。 
“理论”复制协议与实际复制机制之间存在显着差距。例如，复制协议的理论设计通常以过度简化的性能指标(例如消息轮次数)为指导，
而实际的复制机制旨在优化端到端客户端感知的性能指标，例如系统吞吐量，请求处理延迟和故障后的恢复时间。理论设计侧重于复制
协议的单个实例，而实际复制机制必须优化具有多个共存实例的系统的整体性能。

在本文中，我们描述了我们在设计，实现和评估局域网(LAN)集群环境中大规模基于日志的存储系统的复制机制方面的经验。
这种基于日志的设计通常用于存储系统(例如，[12,13,15,24])，通过将随机写入转换为顺序写入并允许批处理以及支持来提高性能事务语义。
许多最近提出的用于基于LAN的集群的存储系统(例如，Petal [19]，Frangipani [28]，Boxwood [21]和Bigtable [7])使用日志。

基于日志的存储系统提供了丰富的结构，提供了有趣的设计选择供探索。例如，复制可以在不同的语义级别和不同类型的目标(例如，逻辑状态，日志或磁盘上的物理数据)上完成。

我们选择设计并实现一个简单，实用且可证明正确的通用复制框架，而不是实现不同的复制机制来探索设计选择。然后，不同的选择将转换为同一复制框架的不同实例。
这显着降低了实施，调试和维护系统的开销，并促进了不同方案的公平比较。

复制框架的设计反映了我们的坚定信念，即可证明正确的复制协议是实际复制机制的重要基础。明确定义的系统模型阐明了协议工作的假设，
并使我们能够理解和评估风险。相比之下，临时复制协议在一些未知的极端情况下失败。

选择适当的复制协议以及选择正确的架构设计是良好实用复制机制的关键。我们相信简单性和模块性是构建实用系统的关键因素。我们的复制框架通过以下功能包含这些原则：  
(i)将副本组配置管理与数据复制分离，Paxos用于管理配置和主/从[2,22]用于数据复制  
(ii)分散式监控检测故障并触发重新配置，监控流量遵循数据复制的通信模式  
(iii)通用和抽象模型，阐明正确性并允许不同的实际实例  

在基于日志的分布式系统环境中，复制框架还使我们能够探索与先前系统(如Boxwood和Bigtable)不同的新复制方案。特别是，我们的方案通过不依赖于单独的锁定服务以及由于更好的数据共址而减少网络流量而在简单性方面提供了优势。
为了评估提议的复制方案，我们实施了PacificA，这是一个基于分布式日志的系统原型，用于存储结构化和半结构化Web数据。 我们已经进行了广泛的评估，以了解各种复制方案的性能和成本，包括单个复制实例和具有多个实例的整个系统。 故障和恢复期间的系统行为在实践中很重要。 我们在这些情况下提供系统行为的详细分析。
本文的结构如下。 第2节描述了我们提出的复制框架。 第3节介绍了可以将框架应用于基于日常的基于日志的存储系统的不同方法。 在PacificA的背景下实施和评估复制方案是第4节的重点。相关工作在第5节进行了调查。我们在第6节中总结。

2. PACIFICA REPLICATION FRAMEWORK
在本文中，我们将注意力集中在基于局域网的集群环境中，其中系统通常由大量服务器组成。每个服务器都有一个或多个CPU，本地内存和一个或多个本地附加磁盘。所有服务器都连接到高速局域网。我们假设一个管理域，其中服务器都是可信任的。没有考虑安全机制。
服务器可能会失败我们假设失败停止失败。我们不假设服务器之间的消息延迟受到限制，尽管在正常情况下它们往往很小。消息可以被删除或重新排序，但不能注入或修改。网络分区也可能发生。服务器上的时钟不一定是同步的，甚至不是松散同步的，但假设时钟漂移是有限的。
系统维护(大)数据集。每个数据都复制在一组服务器上，称为副本组。副本组中的每个服务器都充当副本。服务器可以作为多个组中的副本。数据复制协议遵循主要/备份范例。副本组中的一个副本被指定为主副本，而其他副本称为副主副。副本组的组成(指示主要用户是谁以及辅助用户是谁)被称为副本组的配置。由于副本故障或添加，副本组的配置会发生变化;版本用于跟踪此类更改。
我们专注于实现强一致性的复制协议，因为它代表了最自然的一致性模型。 非正式地，强一致性确保复制系统的行为与非复制系统相同，后者实现了线性化等语义[14]。 应对较弱的一致性模型通常会增加复制层之上构建的应用程序的复杂性。 在第2.7小节中，我们讨论了一种强烈一致性及其含义的放松。
2.1主/备份数据复制
我们采用主要/备份范例进行数据复制。我们区分两种类型的客户端请求：不修改数据的查询和执行更新的更新。在主要/备份范例中，所有请求都将发送到主数据库。主要处理本地所有查询，并涉及处理更新的所有辅助节点。由于几个原因，主要/备用范例在实践中很有吸引力。它很简单，与主要作为唯一接入点的非复制案例非常相似。在许多工作负载中占主导地位的查询直接在单个服务器上处理。
假设更新是确定性的，如果副本组中的所有服务器以相同的顺序处理相同的请求集，则可以实现强一致性。因此，主要用于为更新分配连续且单调递增的序列号，并指示所有辅助站点按此顺序连续处理请求。为清楚起见，我们将副本建模为维护一个准备好的请求列表和一个已列出的列表。该列表是根据分配给请求的序列号进行排序的，并且是连续的(即，只有在插入sn-1后才插入序列号为sn的请求。)准备好的列表的前缀直到提交的点为称为承诺清单。我们在2.6节中描述了抽象模型的实际实现。
主数据库上的应用程序状态是在初始状态下以递增顺序的序列号应用提交列表中的所有请求的结果。保证承诺列表上的请求不会丢失，只要系统不会遇到无法容忍的故障(例如，当前配置中所有副本的永久失败)。使用准备好的列表的未提交的足够的内容确保在重新配置期间保留已提交列表上的请求。我们使用committedp来表示服务器p的提交列表上的更新集，以及用于p的准备列表的preparedp。
正常情况查询和更新协议。在通常情况下(即，没有重新配置)，数据复制协议是直截了当的。当主服务器收到查询时，它会根据当前提交的列表所代表的状态处理查询，并立即返回响应。
对于更新，主p将下一个可用的序列号分配给请求。然后，它将请求及其配置版本和序列号发送到所有副本的准备消息中。在接收到准备消息时，副本r以序列号顺序将请求插入其准备的列表。该请求被认为是在副本上准备的。然后，r在准备好的消息中向主要消息发送确认。当主要收到所有副本的确认时，将提交请求。此时，主要将其提交的点向前移动到最高点，以便到目前为止所有请求都被提交。然后，p将确认发送给客户端，表示成功完成。对于每个准备消息，主节点还会在已提交的点处捎带序列号，以通知辅助节点所有已提交的请求。然后，辅助人员可以相应地向前移动其承诺点。
因为主服务器仅在所有副本将其插入准备好的列表中之后将请求添加到其提交的列表中(当向前移动提交的点时)，主服务器上的提交列表始终是任何副本上准备好的列表的前缀。 此外，由于辅助服务器仅在主服务器执行此操作后才将请求添加到其提交的列表中，因此辅助服务器上的已提交列表将保证为主服务器上的提交列表。 因此，这个简单的数据复制协议支持以下Commit Invariant。
Commit Invariant：设p为主要，q为当前配置中的任何副本，committedq ⊆ committedp ⊆ preparedq成立。
只有在副本组的配置没有变化时，基本主/备份协议才有效。 我们的复制框架将配置管理与数据管理分开。 我们将在下一小节中描述配置管理及其与数据复制的交互。
2.2配置管理
在我们的设计中，全局配置管理器负责维护系统中所有副本组的配置。对于每个副本组，配置管理器维护当前配置及其版本。
当服务器怀疑(通过故障检测)副本有故障时，服务器可以启动重新配置;重新配置将从配置中删除故障副本。服务器还可以建议在配置中添加新的副本;例如，恢复理想的再生水平。在任何一种情况下，服务器都会将建议的新配置及其当前配置版本发送给配置管理器。当且仅当版本与配置管理器上的当前配置的版本匹配时，才会遵守该请求。在这种情况下，建议的新配置将安装下一个更高版本。否则，请求被拒绝。
如果网络分区将主服务器与辅助服务器断开连接，则可能会出现冲突的重新配置请求：主服务器可能要删除某些辅助服务器，而某些辅助服务器会尝试删除主服务器。由于所有这些请求都基于当前配置，因此配置管理器接受的第一个请求获胜。所有其他冲突请求都被拒绝，因为配置管理器已经进入具有更高版本的新配置。
可以使用任何确保后续主要不变量的故障检测机制来触发删除当前副本。在2.3节中，我们详细描述了这种故障检测机制的一种实现方式。
主要不变量：在任何时候，只有配置管理器将p作为其维护的当前配置中的主要内容时，服务器p才会将自身视为主要服务器。因此，在任何时候，最多只有一个服务器认为自己是副本组的主要服务器。
2.3租赁和故障检测
即使配置管理器保持当前配置的真实性，主要不变量也不一定成立。这是因为不同服务器上的配置的本地视图不一定是同步的。特别是，我们必须避免旧主服务器和新主服务器同时处理查询的情况 - 旧主服务器可能不知道重新配置已创建新主服务器并已将其从配置中删除。由于新主数据库可能已处理新更新，因此旧主数据库可能正在处理过时状态的查询，从而违反了强一致性。
我们的解决方案是使用租约[11]。主要通过定期向辅助设备发送信标并等待确认来从每个辅助设备接收租约。如果从最后一个确认信标的发送时间开始经过了指定的租约期，则主要考虑其租约到期。当来自辅助节点的任何租约到期时，主节点不再将自身视为主节点并停止处理查询或更新。在这种情况下，主服务器将联系配置管理器以从当前配置中删除辅助服务器。
只要发件人仍然是当前配置中的主要信号，辅助设备就会确认信标。如果自上次从主服务器接收到信标以来已经过了宽限期，则辅助服务器会将租约考虑到主服务器已过期，并将联系配置管理器以删除当前主服务器并成为新的主服务器。
假设零时钟漂移，只要宽限期与租期相同或更长，租约就会保证在初级到期之前在初级到期。辅助建议配置更改以尝试承担主要角色，当且仅当其到旧主要的租约到期时。因此，保证旧主要在新主要建立之前已经辞职;因此Primary Invariant成立。
我们使用租赁机制作为故障检测机制。其他系统也使用类似的故障检测机制，如GFS [10]，Boxwood和Bigtable / Chubby [5]。这些系统的主要区别在于租赁是从集中实体获得的。在我们的例子中，用于故障检测的监控流量始终位于两个服务器之间，这两个服务器之间已经需要相互通信以进行数据处理：主处理器在处理更新时与辅助设备通信;信标和承认也在主要和次要之间。这样，故障检测准确地捕获了复制所需的通信信道的状态。此外，当通信信道繁忙时，数据处理消息本身可以作为分析和确认。仅当通信信道空闲时才发送实际信标和确认，从而最小化故障检测的开销。此外，这种分散的实现消除了集中式实体的负载和依赖性。负载可能很重要，因为信标和确认在集中式实体和系统中的每个服务器之间定期交换;间隔必须相当小，以便快速检测故障。在集中式解决方案中，集中式实体的不可用性(例如，由于网络分区)可能使整个系统不可用，因为当从中央实体中丢失租约时，所有初选都必须辞职。
![Microsoft-PacificA01.png](https://github.com/Dongzai1005/learning/blob/master/algorithm-theory/src/main/java/wang/xiaoluobo/images/Microsoft-PacificA01.png)
图1：调节：一个例子。 A是旧配置中的主要内容。在新配置中，旧辅助B被提升为新主服务器，其中A已从配置中删除。第一行显示准备好的列表的状态以及基于列表中最高序列号的副本的已提交列表。第二行显示对帐后的相应状态。
形象不变。
重新配置不变：如果新主p在时间t完成协调，则在t之前的任何时间由副本组中的任何副本维护的任何提交列表将保证在时间t处为committedp的前缀。 Commit Invariant保留在新配置中。
重新配置Invariant跨配置扩展Commit Invariant。它还确保分配给任何已提交请求的序列号将被保留，并且即使有主要更改也不会重新分配。相反，不必保留准备列表的未提交部分中的请求。例如，当主要分配该序列号失败时，只要尚未将序列号分配给已提交的请求，新主服务器就可以将相同的序列号分配给不同的请求。辅助节点总是选择来自具有最高配置版本的主节点的分配。
图1说明了主要更改时准备好的列表和已完成的点如何变化。最初，A是B，C和D作为辅助设备的主要部分。请注意，committedB是committedA的前缀，它是任何副本上准备好的列表的前缀。考虑重新配置，将B替换失败的A作为主要。 B完成协调后，新的committedB与旧的preparedB相同，后者包含committedA。已更新preparedC以保留Commit Invariant。对preparedD的额外请求被删除(或撤消)1
增加新的辅助。 可以将新副本添加到副本组，以在组中副本失败后恢复冗余级别。 将新服务器添加到配置时，必须保留Commit Invariant。
注：
1在某些情况下，当D从具有相同序列号的新主要B新请求接收时，可以懒惰地完成预备D的截断。 这些要求将取代现有的要求。

这要求新副本在加入副本组之前具有适当的准备列表.2新副本获取正确状态的过程称为恢复。
一种简单的方法是主要延迟处理任何新更新，直到新副本从任何现有副本复制准备好的列表。这在实践中往往是破坏性的。或者，新副本作为候选辅助加入。主要人员继续处理更新并向候选人的仲裁员发送准备消息。如果主服务器未能从该副本获得确认，则主服务器终止新副本的候选资格。
除了在主要准备消息中记录新更新之外，候选辅助c还从任何现有副本中提取其尚未具有的准备列表的部分。当c最终赶上时，它要求主程序将c添加到配置中。只要c的候选者尚未终止，主服务器就会联系配置管理器，将c作为辅助服务器添加到配置中。经批准后，主要人员将c视为次要的。
在恢复期间将状态转移到新的副本可能是昂贵的。对于从未在副本组中的副本，不可避免地发生完全状态转移。在某些情况下，由于错误的暂停，网络分区或其他瞬时故障而从副本组中删除了副本。当这样一个过时的副本重新加入该组时，理想的情况是让新副本执行追赶恢复[26]，它只获取当副本不在组中时已处理的缺失更新。由于重新配置In-variant，旧副本的提交列表保证是当前提交列表和当前准备列表的前缀。但是，如果旧副本离开组后发生任何主要更改，则旧副本上的准备列表不能保证是当前准备列表的前缀。因此，旧的副本只能将其准备好的列表保留到追赶恢复中的承诺点，并且必须获取其余的副本。

2.5正确性
如果在任何时候配置管理器定义的当前配置中至少存在一个非故障副本，则本节中描述的协议可实现强一致性。我们将复制协议的完整描述及其正确性的正式证明留给单独的技术报告。在这里，我们提供了关于可线性，耐久性和进度的正确性的非正式论证。
可线性化：系统执行相当于所有已处理请求的线性执行。
持久性：如果客户端收到系统确认更新，则更新已包含在等效线性化执行中。
进度：假设非故障服务器之间的通信链路可靠，配置管理器最终响应重新配置请求，并且系统中最终没有故障和重新配置，系统将返回每个客户要求。
对于可线性化，请注意主数据库上的已提交列表定义了所有更新的线性化执行。原始不变量为复制品组定义了一系列原色，每个原型都在非重叠的时间段内起作用。即使主要更改，由于重新配置不变，任何先前主数据库上的已提交列表也可保证为新主数据库上已提交列表的前缀。要向线性化执行添加查询，请观察主要处理由提交列表表示的状态的每个查询;在所提交的列表中的所有已更新更新之后以及之前完成的所有查询之后，自然会添加查询。同样，由于Primary Invariant，在任何时候，单个主要处理所有查询。查询的添加保留了线性化执行。很容易看出线性化执行满足线性化要求的实时排序。
对于持久性，如果客户端收到更新确认，则发送确认的主服务器必须已将请求插入其提交的列表，因此在等效的线性化执行中。
对于Progress，假设配置管理器最终返回每个重新配置请求的响应(Paxos在合理的时序假设下这样做)，副本组最终将重新配置为仅包含非故障副本的配置。此时，将根据数据复制协议处理和确认所有请求。

注：
2实际上，加入服务器会复制当前已提交的状态，以及尚未提交的已准备列表的部分。

2.6实施
实际上，副本不会像抽象模型中所描述的那样维护准备好的列表和提交点。 如何最好地实现它们取决于要复制的数据(或应用程序状态)的语义。 基于复制协议，实现必须有效地支持三种类型的操作。
•查询处理：处理与提交列表相对应的状态的查询。
•对帐期间的状态转移：使副本上的准备好的列表与新的主要列表相同。 由于状态转移，辅助中准备好的列表中的某些未提交的条目可能会被删除。
•恢复期间的状态转移：将准备好的列表所需的部分从副本传输到加入服务器。 作为状态转移的结果，加入服务器上的准备列表中的某些未提交的条目可能会被删除。
允许有效处理查询的一种自然方法是将提交的请求以序列号顺序应用于应用程序状态。准备好的尚未提交的请求可能需要单独维护：这些请求不会直接应用于应用程序状态，因为它们可能需要在状态传输期间撤消：通常，撤消操作很棘手并且可能需要单独执行撤消日志。
对于对帐，新的主要简单地发送所有准备和未提交的请求;它们易于识别，因为它们与应用状态分开维护。为了加速追赶恢复，副本还可以在内存或磁盘上维护最近提交的更新。或者，他们可以通过脏区日志记录标记由最近提交的更新修改的状态段。
在此设计中，要提交更新，辅助服务器必须首先在准备好的列表中记录更新，然后在获知请求已提交时将更新应用于应用程序状态。这通常涉及每个已提交请求的两次磁盘写入。
一种优化是通过将更新首先应用于状态的内存映像并定期将新映像写入磁盘来批量更新到应用程序状态。单独的准备列表不仅保留尚未提交的已准备请求，还保留尚未在磁盘应用程序状态上反映的已提交请求。当内存状态由于断电而丢失时，需要后者来恢复提交的更新。
设计仅附加应用程序状态。在系统维护仅附加应用程序状态的情况下，稍后更新附加到旧状态，而不是覆盖它，可以在仅附加应用程序状态上直接维护准备好的列表。这是因为撤消仅附加状态的更新是直截了当的。
在此设计中，当从主服务器接收请求时，辅助服务器以串行顺序将请求直接应用于应用程序状态，有效地使用应用程序状态来维护其准备好的列表。在提交请求之前，主服务器仍在等待来自所有辅助服务器的知识，有效地使用应用程序状态作为其提交的列表。当从主服务器获取有关提交点的捎带信息时，辅助服务器会懒惰地维护提交的点。
对于协调，当新的主要p开始时，每个次要的s确保在承诺点之后的状态部分与p上的状态部分相同。对于恢复，返回的副本只是从当前主节点复制从其提交点开始的所有内容。
2.7讨论
在本小节中，我们将讨论框架的一些设计细节，并探索替代设计选择。
配置管理器的可用性和性能。让集中式配置管理器简化系统管理，因为它是维护系统中所有副本组配置的当前真相的中心点。更重要的是，分离还简化了数据复制协议，并允许在大小为n的副本组中容忍n  -  1个故障。
对于高可用性，配置管理器本身使用具有标准Paxos协议的复制状态机方法实现。该服务通常部署在少数(例如5或7个)固定服务器上，并且可以容忍少于一半服务器的不可用性。与集中式锁定服务不同，配置管理器的不可用性不会影响其他服务器的正常情况操作。
配置管理器不太可能成为性能瓶颈。配置管理器不参与正常情况请求处理，但仅在重新配置副本组时才参与。通常，Paxos在单轮通信中处理命令，每个成员服务器将更改提交到本地磁盘。可以采取进一步措施来减少配置管理器的负载。通常，对配置管理器的每个请求(即使对于读取)都需要执行协议协议。但是，在我们的示例中，配置管理器的任何成员都可以根据其本地存储的配置版本自由拒绝过时的配置更改请求，因为最新配置至少具有相同版本或更高版本。
瞬态副本组故障期间的耐久性。在前面的部分中，我们展示了复制协议能够容忍n大小的副本组中的n  -  1个失败。在实践中，存在诸如电源故障期间副本组中的所有服务器都发生故障的情况。在这种情况下，在组中的某些副本重新联机之前，副本组将无法取得进展。如果最新配置中的所有副本都永久失败，则会导致数据丢失。如果最新配置中的至少一个副本重新联机，则只要配置管理器也从电源故障中恢复，我们的复制协议允许系统恢复而不会丢失数据。对于使用Paxos协议实现的配置管理器，如果配置管理器的大多数服务器恢复，并且所有Paxos状态都保留在持久存储上，则管理器可以恢复。
为了实现对整个副本组的瞬时故障的持久性，所有副本都在持久存储上维护其准备好的列表和提交点。当副本从故障中恢复时，它会联系配置管理器以查看它是否仍然是当前配置中的副本。如果是这样，它将承担当前的角色：如果它是主要角色，它将尝试从次要角色重新获得租约;如果它是次要的，它将响应来自当前主要信标的信标。检测到故障时，服务器启动重新配置。
主要/备份与Paxos。使用主要/备份范例的替代方案是采用Paxos范例。
在Paxos中，只有在法定人数(通常是大多数)副本上准备好请求后才会提交请求。即使对于查询请求也是如此，尽管我们的协议中的相同租赁机制可用于允许对包含仲裁的租约的单个副本进行查询。
在主要/备份范例中，只有在所有副本上准备更新请求后才会发出更新请求，而不是像在Paxos中那样以多数形式准备更新请求。两种协议中不同的法定人数选择有几个含义。
首先，Paxos对与少数复制品相关的瞬态性能问题不敏感：它能够以最快的多数提交请求;其他人可以赶在后台。同样，少数复制品的失败对性能影响最小。
相反，在我们的主/备份协议中，任何副本的缓慢都会导致协议速度变慢。在重新配置从副本组中删除该副本之前，任何副本的故障都会停止进度。必须适当设置租约的超时值：较高的值会在副本故障期间转换为较高的中断时间，而较低的值会导致可能的错误怀疑，从副本组中删除无故障的慢副本。虚假的怀疑会降低系统的弹性;我们的协议便于快速赶上恢复，以便复制品快速重新加入，从而减少漏洞窗口。
其次，当大多数副本不可用时，Paxos无法处理任何请求。但是，在我们的协议中，副本组可以在配置管理器的帮助下重新配置，并且只要至少有一个副本可用，它就会继续运行。
第三，在单独的配置管理器的帮助下，我们的协议中的重新配置很简单。使用Paxos，副本组可以通过将重新配置视为当前配置中的共识决策来重新配置自身。新副本必须联系当前配置中的多数，才能正确传输状态。
值得商榷的是，哪种选择在实践中提供了更好的权衡。我们选择主要/备用范例主要是为了它的简单性。即使Paxos范式被采用，我们报告的大多数经验和结果仍然有效。
削弱强一致性。强一致性基本上要求两个要求。首先，所有副本必须以相同的顺序执行同一组更新。第二，查询返回最新状态。放松强烈的一致性可能会有好处。放宽第一个要求会导致复制品之间的状态转移，这通常要求系统检测并恢复到一致状态，并且还要求客户应对不一致。此类系统的示例包括Coda [16]，Sprite [3]，Bayou [27]，CONIT [30]和PRACTI [4]以及GFS。
我们反而讨论第二种放松;也就是说，我们允许查询返回一个有点过时的状态。这有两个有趣的含义。首先，复制协议在不依赖于租赁机制的情况下实现了这种弱化语义：请注意，主要不变量是唯一依赖于租赁机制中的时序假设的不变量。没有这个不变量，旧的主要人员可能会错误地认为它仍然是过时状态的主要和进程查询。但是，两个并发原色的存在不会导致不同副本上的冲突状态。这是因为我们的重新配置协议规定新主服务器是先前配置中的副本。鉴于只有在配置中所有副本的同意之后才提交更新，只要新主服务器在旧配置中不再充当辅助服务器，则在新主服务器获得批准后，将无法再提交更新。配置管理员。因此，即使在初级变化期间也不会发生状态分歧。
第二个含义是辅助人员也可以处理查询。由于Commit Invariant和Reconfigura-Invariant，辅助服务器上的已提交列表表示有效但过时的状态。

图2：基于日志的存储系统架构。
![Microsoft-PacificA02.png](https://github.com/Dongzai1005/learning/blob/master/algorithm-theory/src/main/java/wang/xiaoluobo/images/Microsoft-PacificA02.png)

3。
对分布式日志的复制 - 
基于存储系统
我们通过基于日志的分布式存储系统探索所提出的复制框架的实际方面。
图2显示了(非复制)基于日志的存储系统的一般体系结构。系统维护应用程序数据的磁盘映像以及捕获磁盘映像更新的内存数据结构。为了保持一致性，系统还会为仅在内存中维护的更新保留应用程序日志。
当系统接收到更新时，它将请求写入日志以保持持久性(步骤1)并将请求应用于内存中数据结构(步骤2)。重新播放日志以在服务器重新启动时重建内存中状态。周期性地(在内存数据结构耗尽存储器之前)，为存储器内数据结构创建磁盘上的检查点(步骤3)。创建检查点后，可以从日志中截断已在磁盘检查点中反映的更新的所有日志条目(步骤4)。系统可以选择存储一个或多个检查点。定期地，检查点中反映的更新与磁盘上图像合并以创建新的磁盘上图像(步骤5)。系统可以选择合并而不检查点：内存数据结构中反映的更新直接与磁盘映像合并。我们选择更通用的形式，因为它很好地涵盖了特殊情况，因为它准确地反映了一些实际的实际系统(如Bigtable)的工作，并且因为在某些情况下维护检查点可以加快协调。使用内存数据结构，磁盘检查点和磁盘映像提供查询。值得指出的是，这种基于日志的设计消除了随机磁盘写入并将其转换为更高效的顺序磁盘写入。
3.1逻辑复制
在第一个复制策略中，复制只是应用于整个系统状态：每个副本支持与原始非复制系统相同的接口，并且能够处理相同类型的更新和查询。这是合乎逻辑的，因为副本上的状态在逻辑上是相同的。副本可能保持不同的物理状态;副本可以单方面决定何时检查点以及何时合并。
对于逻辑复制，因为应用程序状态不是仅附加，我们选择将准备好的列表与应用程序状态分开维护。重要的是，我们可以通过维护既作为应用程序日志又作为准备好的请求的存储的日志，来消除将更新写入准备好的列表的成本。为此，我们使用三个字段扩充包含客户端请求的应用程序日志条目：配置版本，序列号和最后提交的序列号。日志条目使用具有相同序列号和较低配置版本的任何其他条目;在主要更改期间撤消准备好的请求时会发生这种情况。因此，应用程序日志包含具有已提交序列号的所有非包含日志条目，而未提交的日志条目属于准备好的列表。
在第一阶段，当收到包含请求，配置版本，序列号和最后提交的序列号的准备消息时，副本会将消息附加到日志中。在第二阶段，当提交请求时，它将应用于内存数据结构，但应用程序不再需要写入日志：请求已经在log.3中。
应用程序在检查点后截断其日志。但是只截断了与提交和检查点更新相对应的日志条目。我们的复制协议只需要准备好的列表的已提交后缀。截断期间确实保留了所有这些条目。
每个检查点都反映了一系列序列号的更新;范围与检查点相关联。我们还将磁盘映像与它反映的最后一个序列号相关联。这些相关的序列号可以帮助确定追赶恢复期间需要哪个状态。

注：
3在主数据库上，尽管已提交的请求立即应用于内存数据结构，但最后提交的序列号会在日志中延迟更新。 这不会影响正确性，因为我们的协议确保所有副本的准备列表中的任何请求都不会丢失。

逻辑复制的变种。处理内存数据结构的更新，生成检查点以及合并所有消耗资源(例如CPU周期和内存)，并且可能很昂贵。在逻辑复制中，这些操作既可以在辅助节点上完成，也可以在辅助节点上冗余地完成。
可能最好只让主要执行这些操作。然后可以在所有副本上复制这些操作的结果。因此，我们引入了逻辑复制方案的变体，称为逻辑-V。在logical-V中，只有主服务器维持内存中状态。辅助服务器在从主服务器接收更新时，只需将请求附加到日志，如逻辑复制，而不将请求应用于内存中状态。辅助服务器会在生成检查点时从主服务器获取检查点。只有当条目对应的更新被丢弃或提交并反映在本地检查点时，才能发出辅助节点上的日志条目。
Logical-V代表了逻辑复制的一个有趣的权衡。在logical-V中，辅助节点消耗更少的内存(因为它们不再维持内存结构)和更少的CPU(因为它们不再生成检查点，这涉及压缩。)但是，逻辑-V确实会产生更高的网络负载，因为检查点现在在副本之间传输。更重要的是，逻辑-V中的主要故障转移更耗时：要成为新的主要，次要必须重放日志以重建内存状态并生成所需的检查点。
3.2分层复制
基于日志的系统中的持久状态包括检查点，磁盘上映像和日志。它们都可以作为文件实现。然后可以将基于日志的存储系统视为由两层组成，其中下层提供持久文件存储，上层将应用程序逻辑转换为文件操作。
分层视图导致另一种设计，它将复制分别应用于两个层。在较低层，简单的仅附加文件抽象足以支持诸如附加新日志条目和创建新检查点之类的操作。因此，我们可以将第2.6节中概述的仅附加状态的设计用于文件复制。
具有较低层文件复制也是有吸引力的，因为抽象是可重用的并且简化了上层的逻辑。 RAID是这种设计的典型例子。最新的分布式存储系统，如Petal上的Frangipani，RLDev上的Boxwood和GFS上的Bigtable，都使用分层方法，并在下层具有主复制逻辑。
在上层，对于每个数据，一些所有者服务器负责接受客户端对数据的请求，维护内存中结构，并写入(附加)到较低层的文件以保持持久性和可靠性。例如，要写入日志条目，上层只是请求下层将条目附加到日志文件。上层不关心日志的维护位置，是否复制，或者如何复制日志。无论文件是日志还是检查点，下层都不知道或不关心正在操作的文件的语义。
低层复制并不能完全消除上层容错的需要：当所有者服务器出现故障时，新的所有者服务器必须接管其责任。在以前的系统中，使用单独的分布式锁定服务来确保在每个数据的上层选择唯一的所有者服务器。我们做的一个重要观察是，不是发明和部署新机制，而是可以在上层重用相同的复制逻辑，以选择唯一所有者来接受和处理客户端请求：选择唯一所有者的问题是与实现Primary Invariant的复制框架的配置管理部分相同。
自然设计是选择管理较低层数据的副本组的主要服务器作为所有者服务器。有两个主要优点。首先，我们从较低层的复制协议获得上层的“免费”故障转移机制，没有分布式锁服务，也不会产生任何额外开销。当所有者服务器发生故障时，将在较低层选择新的主服务器，并将成为新的所有者服务器。其次，这种安排确保数据的副本始终驻留在所有者服务器上，从而减少网络流量。
3.3日志合并
系统通常维护多个副本组。正如GFS和Chain Replication [29]所倡导的那样，服务器参与一组不同的副本组，以便在服务器出现故障时分散恢复负载。鉴于逻辑上存在与每个副本组相关联的日志，每个服务器必须维护多个这样的日志。使用日志记录的好处可能会减少，因为写入多个日志会导致磁盘头定期搜索。
一种解决方案是将多个日志合并到单个物理日志中，如Bigtable中所做的那样。选择复制的方式会影响日志的合并方式。对于逻辑和逻辑V，日志条目显式发送到辅助节点。然后，每个服务器可以将日志条目放在单个本地物理日志中。生成的本地物理日志包含服务器所属的所有副本组的日志条目，无论服务器是组的主服务器还是辅助服务器。在此方案中，系统中的每个服务器只有一个本地物理日志。
对于分层复制，服务器将其拥有的所有数据的日志条目写入下层的单个复制日志。这有效地合并了服务器是主服务器的所有副本组的日志，但不合并服务器作为辅助服务器的副本组的日志。
在系统中有n个服务器，每个服务器都是某个副本组的主服务器，在分层方案中，将有n个重复日志。如果每个日志被复制k次，则系统中将存在单独的本地日志。这与逻辑和逻辑V情况中的n个本地日志形成对比。在分层方案中，无法保证故障转移期间新主节点所需的日志条目是本地的。这是因为不同副本组的日志条目合并为一个日志。这些不同的副本组通常在不同的服务器上具有辅助副本以启用并行恢复。因此，日志的辅助副本不能与所有这些辅助副本共存。相反，在逻辑V情况下，在故障转移期间，所需的所有日志条目都保证是本地的，但每个服务器必须解析合并的日志文件以查找与新的主副本相关的必要日志条目。对于逻辑复制，副本重播其本地日志以在副本从断电重新启动时重建内存中状态，但新主节点在故障转移期间不需要重播其本地日志，因为它维护内存中状态。

4.评估
为了评估复制框架并评估其在实际基于日志的分布式存储系统中的工作情况，我们实施了PacificA，这是一个基于日志的原型存储系统。此处仅描述与复制相关的功能。 PacificA实现了一个分布式存储系统，用于存储大量记录。每条记录都有一个唯一的密钥;为所有键定义总订单。记录被分成多个切片，每个切片由连续键范围内的所有记录组成。 PacificA为客户提供了一个接口，用于扫描给定密钥范围内的记录，查找特定密钥的记录，使用特定密钥添加新记录，或修改/删除现有记录。
PacificA设计遵循图2中描述的架构，并采用Bigtable中的各种功能。在切片的磁盘映像中，切片中的记录将按其键的排序顺序连续存储。保留索引结构和Bloom过滤器以进行快速查找。记录所有修改并将其反映在内存数据结构中。内存数据结构也基于密钥进行排序。当内存数据结构增长到一定大小时，数据结构中的信息被压缩并以相同的排序顺序写入持久存储。在将检查点与磁盘映像合并时，排序顺序允许简单的合并排序。为减少写入磁盘的数据量，ZLib(压缩级别1)用于压缩检查点和磁盘映像中的数据。
我们已经完全实现了三种不同的复制策略：逻辑复制，逻辑V复制和分层复制。由于使用相同的复制框架，代码库的很大一部分对于不同的策略是通用的。
评估是在通过48端口1GB以太网交换机连接的28台机器的集群上完成的。其中16台机器用作PacificA服务器;其他用作客户端机器。这16台服务器机器均配备2.0 GHz Intel Xeon双核CPU，4096 MB内存，1Gb以太网卡和232 GB SATA硬盘。在我们的实验中，我们让配置管理器在单独的机器上运行非复制。我们将租约期设定为10秒，宽限期设定为15秒。
由于查询仅由主服务器提供，因此复制方案的评估主要侧重于更新的性能。为了生成具有更新的工作负载，客户端随机生成一个26字节的密钥和一个32 KB的记录，压缩率约为65％。大小和压缩比模仿我们拥有的一组100 GB爬网页面。我们已经尝试了将URL作为键插入真实网页数据的工作量，并找到了可比较的结果。合成工作负载允许客户端有效地生成负载(无需从磁盘加载网页并首先解压缩数据)以使服务器饱和。对于报告更新吞吐量的所有实验，我们总是有足够的客户端，有足够的线程发送随机生成的更新以使服务器饱和。组提交在所有方案中实施，默认情况下处于启用状态。副本组的默认副本数为3。

4.1正常情况表现
在第一个实验中，客户端在存储在三个服务器的副本组上的单个片上发送更新。在此实验中不执行合并操作，并且禁用组提交。表1显示了所有三种复制方案的客户端观察到的吞吐量，以及复制品在CPU利用率，磁盘吞吐量和网络利用方面的内部复制成本。为了进行比较，非复制设置的客户端吞吐量为23.7 MB / s，由于缺少复制成本，比逻辑V的客户端吞吐量高约10％。我们显示主要和次要的资源消耗。报告的数字是10分钟内的平均值。
两个二级资源的资源消耗实际上是不同的。这主要是因为我们的实现使用链式结构为主要发送请求到次级，如GFS中所做的那样：主要将数据发送到第一个辅助节点，然后将数据进一步传送到下一个辅助节点。这种结构减少了主要用于发送数据的开销，并以更平衡的方式利用网络带宽。在这样的实现中，链中的第一个辅助节点必须将数据转发到下一个辅助节点，从而产生比下一个辅助节点更多的开销。差异不显着。报告的数字是第一个中学。
对于所有这三种方案，磁盘和CPU都接近饱和状态。 Logical-V的吞吐量略高，从而导致更高的CPU利用率，网络吞吐量和磁盘吞吐量。分层似乎受到轻微影响，因为它必须将日志条目传递到较低层进行传输，从而中断请求处理流水线操作。客户端观察到的吞吐量低于repli-上的磁盘吞吐量
cas因为每条记录不仅写入日志，而且还压缩并写入检查点。 Logical-V和分层复制在辅助节点上具有较低的CPU利用率，因为辅助节点不维护内存结构或执行检查点。这两种方案的网络吞吐量较高，因为它们需要将检查点发送到辅助站点。

table1
![Microsoft-PacificA08.png](https://github.com/Dongzai1005/learning/blob/master/algorithm-theory/src/main/java/wang/xiaoluobo/images/Microsoft-PacificA08.png)


图3：检查点和合并，逻辑复制的影响。
![Microsoft-PacificA03.png](https://github.com/Dongzai1005/learning/blob/master/algorithm-theory/src/main/java/wang/xiaoluobo/images/Microsoft-PacificA03.png)

时间观点。在第二个实验中，我们研究了由于检查点和合并而在基于日志的系统中固有的时间波动。图3显示了主要的CPU利用率，网络吞吐量和磁盘读/写吞吐量，随着时间的推移进行逻辑复制。
磁盘读取吞吐量(秒30和135)的两个高程标记了读取检查点和磁盘映像的合并操作的开始。 Merge还引入了对日志写入的竞争磁盘写入。结果是更慢的日志写入和客户端请求处理。这转化为较低的CPU利用率(由于处理请求较少)，较低的网络吞吐量和较低的磁盘写入吞吐量(即使有额外的写入请求源。)第一次合并在第二次115结束，此时系统能够处理更多客户端请求，反映在CPU利用率，网络吞吐量，磁盘写入吞吐量的大幅增加，这都是由于客户端请求处理造成的。
检查点定期发生，从第二个10开始，即使在合并操作期间也是如此。执行检查时，由于压缩，CPU利用率会上升。检查指针还写入磁盘，与日志写入竞争。这会减慢日志写入和客户端请求处理速度。结果是降低了磁盘吞吐量和网络吞吐量。因此，每个小的波动对应于检查指向动作。
逻辑V和分层复制的图形是类似的，因此被省略。唯一明显的区别是，在检查点期间，这两种方案的网络吞吐量更高，因为检查点被运送到辅助站点。
可扩展性。在正常情况的最终实验中，
我们研究了系统的可扩展性。我们将分配给每个服务器的切片数量修复为40，并使用随机副本放置策略，为服务器提供切片和主要/次要角色的均衡分配。然后，我们使用系统中不同数量的服务器，以每秒(32KB)记录的数量来衡量聚合客户端吞吐量。图4显示了所有这些方案的近乎完美的可扩展性。由于逻辑复制利用了辅助资源上的额外资源，因此在具有多个副本组的端到端实验中，其整体吞吐量会降低。分层复制较低，因为在该方案中每个服务器上有三个日志，并且因为观察到它对加载不均衡更敏感：过载的服务器似乎会影响分层复制中的更多服务器。
我们进一步衡量随机密钥查询的可扩展性。 每个服务器是一个切片的主要服务器，具有大约3GB的数据，在大约56个检查点中组织。 我们关闭自己的缓存，但不关闭文件系统缓存或磁盘缓存。 我们报告稳定状态下的查询吞吐量。 由于查询仅由原色处理，并且具有相同的数据放置，因此所有三种复制方案都会产生相同的结果。 因此，我们只报告一系列数字。 由于数据协同定位，每个查询请求在本地处理，这导致接近完美的缩放。

图4：可伸缩性
![Microsoft-PacificA04.png](https://github.com/Dongzai1005/learning/blob/master/algorithm-theory/src/main/java/wang/xiaoluobo/images/Microsoft-PacificA04.png)


图5：故障，协调和恢复期间的性能。
![Microsoft-PacificA05.png](https://github.com/Dongzai1005/learning/blob/master/algorithm-theory/src/main/java/wang/xiaoluobo/images/Microsoft-PacificA05.png)

4.2故障，重新调整和恢复期间的性能
在本小节中，我们将报告失败，对帐和恢复期间的评估结果。第一个实验在三个存储单个切片的服务器上使用单个副本组。图5显示了面对以下一系列事件时客户端感知吞吐量(就每秒记录数而言)的变化。在第二个60，主要被杀死;服务器在第二次160时恢复并重新加入;同样的服务器，现在是次要服务器，在第二个410再次被杀死;它在第二次490再次恢复并重新加入。
图5显示了逻辑复制和逻辑V复制随时间的结果。尽管日志处理存在差异，但分层复制与逻辑-V几乎完全相同，因此仅显示逻辑-V曲线。 (下一个实验区分逻辑V和分层复制。)
在第二个60岁，小学生被杀死。租约在15秒内到期，此时辅助服务器提出新配置以删除故障主服务器。逻辑复制中的重新协调速度要快得多，因为仲裁已经维护了内存中的数据结构。这意味着更短的停机时间。对于逻辑V复制，新辅助节点必须加载日志并播放日志以重新构建内存中的数据结构，然后才能接受新请求。
在第二次160，故障服务器恢复并重新加入。由于新副本的追赶恢复导致的资源消耗导致客户端感知的吞吐量降低。在第二个300完成恢复时，副本组将还原到3个服务器。当作为辅助服务器的服务器在第二410被杀死时，在主要重新配置以移除故障辅助服务器之前需要10秒。但是不需要重新调整。因此，我们看到较小的中断。由于服务器恢复并重新加入时的追赶恢复，观察到吞吐量的类似下降。
在重新配置的点处存在波动(例如，在第350和650秒处添加新的辅助设备时)，尤其是对于逻辑复制。这是因为当主要联系配置管理器以进行重新配置时，会缓冲请求。批准重新配置时，将批量处理这些缓冲的请求。由于缺乏对事件时间的精确控制，两个图形不完全对齐。
故障转移时间。在下一个实验中，我们将研究不同方案的故障转移时间。在分层复制中，故障转移发生在两个层：在较低层，准备但尚未提交，请求被重新发送。这相对较快，因为这些请求在内存中。上层的故障转移涉及获取和播放日志以重建内存中的数据结构。后者更昂贵，取决于需要重播的日志的大小。
在Logical-V中，故障转移还涉及新主服务器上的日志处理，但处理逻辑与分层复制的处理逻辑不同。当每个服务器上有多个切片时，差异更明显。我们使用三服务器三片配置来量化关于日志大小的故障转移时间，其中每个服务器是片的主要部分，另外两个是辅助服务器。然后我们杀了一台服务器;然后，另一台服务器成为最初分配给故障服务器的片的主服务器。我们使用不同的日志大小来衡量故障转移时间。
图6显示了三种方案的结果。逻辑复制执行得最好，因为新主服务器不必重放日志以重建内存数据结构。 Logical-V在本地重放日志：由于日志合并，其本地日志大约是要恢复的片的日志的三倍。对于分层复制，指定的服务器会加载日志条目并将它们发送到新的主服务器。这导致远程磁盘读取，这通常是不可避免的：失败的服务器通常是多个切片的主要部分;指定的服务器需要处理日志并将相关的日志条目发送到不同的新原色。但由于日志仅包含要恢复的片的日志条目，因此分层复制中读取的总字节数仅为logical-V中的1/3。
然而，由于远程读取，在该实验中分层仍然较差。
逻辑V和分层复制之间的比较并不像结果所表明的那样简单。对于logical-V，如果每个服务器是n个切片的主要服务器，则每个服务器上的日志平均包含3n个切片的条目。新主服务器必须读取整个日志才能获取一小部分有用的日志条目。我们维护一个内存映射来定位每个片的日志条目，从而避免读取整个日志。当服务器出现故障时，最多可能有n台服务器重放其本地日志。对于分层复制，仅解析单个日志，并将相关日志条目发送到适当的新原色。它从磁盘整体读取的数据较少，但可能会导致更高的延迟，因为一台服务器必须处理整个日志。
失败和复苏的全球影响。为了量化真实环境中故障的影响，我们还研究了两台服务器被杀死时16机群集中的系统性能。图7显示了随时间变化的吞吐量变化(以每秒记录数计)。在这个实验中，两个服务器在大约五分钟内被杀死。对于分层复制，协调持续5分钟，并在分钟10结束。我们等到所有协调完成后再指示丢失副本的副本组添加随机服务器以恢复冗余级别。此过程导致更低的吞吐量并在24分钟结束。系统然后返回到正常状态，吞吐量略低于故障之前的吞吐量，因为只剩下14个服务器。逻辑复制几乎立即完成协调，并在大约8分钟完成恢复：恢复速度更快，因为此时系统中插入的数据更少。逻辑-V的曲线类似于逻辑复制的曲线。这有点令人惊讶。用于在合并日志上定位切片的日志条目的内存映射显着减少了新原色在逻辑V中读取的数据量。此外，全局协调在所有服务器上并行执行，并立即完成。这与分层复制的情况形成对比，分层复制需要解析两个2 GB的日志。
简介：总的来说，我们已经证明复制框架确实产生了实用的复制方案，这些方案在正常操作期间以及在故障和恢复期间都具有良好的性能，以及良好的可伸缩性。 在我们评估的三种方案中，逻辑复制在故障转移期间明显优越。 这是以额外的CPU和内存资源为代价的，这导致正常情况下的性能较差，尤其是在完全运行CPU和内存的工作负载中。 由于对日志的特殊处理以及在故障转移期间处理日志的额外机制，分层复制更加复杂。 复杂性似乎也转化为性能稍差。 Logical-V更简单，并提供合理的整体性能。
5。
相关工作
已经以共识协议和复制的状态机方法的形式广泛研究了复制协议。代表性协议包括Paxos [18]和视图标记复制[22]。
分布式系统的历史悠久，可实现高可靠性和可用性的复制。很难一一列举它们。相反，我们专注于与我们最接近的一小部分解决方案。
HARP [20]是最早在商用硬件上构建高可用性文件系统的人之一。它采用主备份进行复制，并具有预写日志。 HARP的日志完全保留在内存中，并在提交请求之前复制到所有仲裁。 HARP不需要维护磁盘日志，因为它假定每台服务器都配备了UPS。 HARP使用视图标记复制协议[22]。每个副本组由2n + 1个服务器组成，其中n + 1个服务器存储数据。其他n个服务器只是证人，并且只在重新配置(即查看更改)时参与。
与HARP一样，Echo [26]也在证人的帮助下使用主要/备份。 Echo查看复制的不同语义级别，尽管所有选项都处于文件系统级别且较低级别。日志用于日记，解决方案类似于PacificA中的逻辑V复制，尽管Echo不考虑日志合并。 Echo的设计考虑了和解和恢复。 Echo还为客户端和副本使用租赁机制。
Petal [19]实现了高度可用的分布式虚拟磁盘。 Petal是第一个使用Paxos实施全球州经理的人。数据复制采用镜像。每个副本组的配置都保存在全局状态管理器中。对于协调和恢复，该方案使用忙位来跟踪正在进行的操作，并使用过时位来跟踪仅由一个副本更新的数据区域。 Boxwood [21]使用类似的技术来实现低级复制逻辑设备。与在所有服务器上实现全局状态管理器的Petal不同，Boxwood使用一小组服务器。两个系统仅支持镜像。
Frangipani [28]是一个分布在Petal之上的分布式文件系统。 Frangipani使用元数据记录来实现干净的故障恢复并提高性能。虽然没有考虑日志合并，但我们的分层复制中的日志仍保留在Petal中。分布式锁服务用于协调对Petal存储的访问。分布式锁服务本身使用实现Paxos的全局状态管理器。在Boxwood中使用类似的基于锁服务器的设计，在低层可靠存储之上层叠高级B-Link树抽象。在这两种情况下，不协调不同的层以进行共址。预计工作负载将显示具有非常小的冲突请求的位置，以使缓存有效。
Google文件系统(GFS)[10]是专为Google工作负载而设计的分布式文件系统，其中文件很大且主要仅附加。 GFS使用基于日志的方法在主服务器上维护元数据;主服务器在主/备份方案的变体中复制。对于数据复制(在块服务器上)，GFS采用宽松一致性模型，其中复制品在发生故障时可能具有不一致的状态。宽松一致性模型允许复制协议不明确地处理重新配置和对帐。此选择简化了复制协议，但代价是增加了客户端处理不一致的复杂性。
Bigtable [7]是一个分布在GFS之上的分布式结构化存储。 PacificA的设计受到Bigtable的显着影响，但也存在显着差异。 Bigtable的持久状态存储在GFS中;这包括日志和SSTable(对应于图2中的检查点和磁盘映像)。在上层，BigTable使用的丘比锁服务[5]，以跟踪管理器(或服务器片剂)，其中管理器必须以对GFS国家相关操作OB-覃的排他锁的。 Bigtable的平板电脑大师和他们的平板电脑数据之间的共处可以尽力而为。然而，分层设计是一个明智的工程选择，因为GFS在Bigtable建成时已经可用。
链复制[29]将副本组中的副本组织成链。更新被发送到链的头部并沿着链处理，直到它们被提交到尾部。尾部响应查询和更新。链复制还使用Paxos来实现配置管理器。链复制可以被视为我们复制框架的变体。将尾部视为我们框架中的主要部分，链复制提供了另一种实现重新配置不变和提交不变的方法。链复制假定没有网络分区，但即使在网络分区期间也可以使用相同的租用机制来实现主要不变量。每个副本都可以简单地保持其链上的前身的租约。
以增加的延迟为代价，通过拆分查询并更新头部和尾部之间的负载，链复制受益。但是，当服务器参与具有不同角色的多个副本组时，优势会降低。对于分层复制，因为所有者服务器执行维护内存数据结构和执行检查点/合并的额外工作，所以在较低层使用链复制无助于从所有者服务器卸载开销。尽管我们的实验结果和Chain Replication的模拟结果之间的直接和公平的比较很难，但根据我们的经验，我们认为链复制提供与我们的主要/备份对应的可比吞吐量，如果不是更糟的话。
与先前使用主要/备份的提案不同，美联储砖块(FAB)[9]举例说明了基于仲裁的替代方法。 FAB使用新的基于多数投票的方案实现可靠且可扩展的分布式磁盘阵列，允许重新配置。重新配置是通过类似于Paxos的动态投票方案完成的，并且需要状态同步。
许多对等存储系统，如CFS [8]和PAST [回到顶部](#[REFERENCES-1])[25]，也采用复制来提高可靠性。大多数此类系统存储不可变数据，这极大地简化了复制逻辑。 Oceanstore [17]和FARSITE [1]等系统确实允许数据突变。由于在不受信任的P2P环境中可能存在恶意攻击，两个系统都使用拜占庭容错协议(例如，[6]。)
6。
结束语
复制方案是分布式系统的重要组成部分，在这些系统的性能和可靠性方面发挥着重要作用。 本文提供了一种简单而干净的思考复制方法及其在通用复制框架环境中的正确性的方法。 与此同时，我们展示了如何跨越桥梁，从可证明的正确方案到实际的复制方案。 我们的经验为基于日志的存储系统的复制以及复制处理故障的方式如何影响系统行为带来了新的见解。

REFERENCES-1

7. REFERENCES
[1] A.Adya,W.J.Bolosky,M.Castro,G.Cermak,R.Chaiken,J.R. Douceur, J. Howell, J. R. Lorch, M. Theimer, and R. P. Wattenhofer. FARSITE: Federated, available, and reliable storage for an incompletely trusted environment. In Proc. of OSDI 2002, pages 1–14, December 2002.
[2] P.AlsbergandJ.Day.Aprincipleforresilientsharingofdistributed resources. In Proc. of the 2nd International Conference on Software Engineering, pages 627–644, October 1976.
[3] M.BakerandJ.K.Ousterhout.AvailabilityintheSpritedistributed file system. Operating System Review, 25(2):95–98, April 1991.
[4] N.Belaramani,M.Dahlin,L.Gao,A.Nayate,A.Venkataramani, P. Yalagandula, and J. Zheng. PRACTI replication. In Proc. of 3rd NSDI, May 2006.
[5] M.Burrows.Thechubbylockserviceforloosely-coupled distributed systems. In Proc. of the Seventh Symposium on Operating System Design and Implementation (OSDI 2006), November 2006.
[6] M.CastroandB.Liskov.Practicalbyzantinefaulttolerance.In Proc. of the 3rd OSDI, February 1999.
[7] F.Chang,J.Dean,S.Ghemawat,W.C.Hsieh,D.A.Wallach, M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber. Bigtable: A distributed storage system for structured data. In Proc. of the 7th OSDI, November 2006.
[8] F. Dabek, M. F. Kaashoek, D. Karger, R. Morris, and I. Stoica. Wide-area cooperative storage with CFS. In Proc. of the 18th SOSP, pages 202–215, 2001.
[9] S.Frølund,A.Merchant,Y.Saito,S.Spence,andA.Veitch.FAB: Building reliable enterprise storage systems on the cheap. In Proc. 11th ASPLOS, October 2004.
[10] S.Ghemawat,H.Gobioff,andS.T.Leung.TheGooglefilesystem. In Proc. of the 19th SOSP, October, 2003.
[11] C. Gray and D. Cheriton. Leases: An efficient fault-tolerant mechanism for distributed file cache consistency. In Proc. 12th Symp. Operating Systems Principles (SOSP), pages 202–210, December 1989.
[12] J.Gray.Notesondatabaseoperatingsystems.OperatingSystems: An Advanced Course, Lecture Notes in Computer Science, 60:393– 481, 1978.
[13] R.Hagmann.ReimplementingtheCedarfilesystemusinglogging and group commit. In Proc. of 11th SOSP, pages 155–162, 1987.
[14] M. P. Herlihy and J. M. Wing. Linearizability: a correctness condition for concurrent objects. ACM Trans. Program. Lang. Syst., 12(3):463–492, 1990.
[15] M.Kazar,B.Leverett,O.Anderson,V.Apostolides,B.Buttos, S. Chutani, C. Everhart, W. Mason, S. Tu, and E. Zayas. Decorum file system architectural overview. In Proc. of the USENIX Summer’90 Conference, pages 151–163, 1990.
[16] J.J.KistlerandM.Satyanarayanan.Disconnectedoperationinthe Coda file system. In Proc. of the 13th SOSP, pages 213–225, 1991.
[17] J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski, P. Eaton, D. Geels, R. Gummadi, S. Rhea, H. Weatherspoon, W. Weimer, C. Wells, and B. Zhao. OceanStore: An architecture for global-scale persistent storage. In Proc. of 9th ASPLOS, pages 190–201, November 2000.
[18] L.Lamport.Thepart-timeparliament.ACMTrans.onComputer Systems, 16(2):133–169, May 1998.
[19] E.K.LeeandC.A.Thekkath.Petal:Distributedvirtualdisks.In Proc. 7th ASPLOS, pages 84–92, October 1996.
[20] B.Liskov,S.Ghemawat,R.Gruber,P.Johnson,L.Shrira,and M. Williams. Replication in the Harp file system. In Proc. of 13th SOSP, pages 226–238, 1991.
[21] J.MacCormick,N.Murphy,M.Najork,C.A.Thekkath,and L. Zhou. Boxwood: Abstractions as the foundation for storage infrastructure. In Proc. of the 6th OSDI, pages 105–120, December 2004.
[22] B. M. Oki and B. H. Liskov. Viewstamped replication: a new primary copy method to support highly-available distributed systems. In Proc. of 7th PODC, pages 8–17, 1988.
[23] C.H.Papadimitriou.Theserializabilityofconcurrentdatabase updates. Journal of the ACM, 26(4):631–653, October 1979.
[24] M.RosenblumandJ.K.Ousterhout.Thedesignand implementation of a log-structured file system. ACM Trans. on Computer Systems, 10(1):26–52, 1992.
[25] A.RowstronandP.Druschel.Storagemanagementandcachingin PAST, a large-scale, persistent peer-to-peer storage utility. In Proc. of the 18th SOSP, pages 188–201, 2001.
[26] G.Swart,A.Birrell,A.Hisgen,andT.Mann.Availabilityinthe Echo file system. Research Report 112, System Research Center, Digital Equipment Corporation, September 1993.
[27] D.B.Terry,M.M.Theimer,K.Petersen,A.J.Demers,M.J. Spreitzer, and C. H. Hauser. Managing update conflicts in Bayou, a weakly connected replicated storage system. In Proc. of the 15th SOSP, pages 172–183, December 1995.
[28] C.A.Thekkath,T.P.Mann,andE.K.Lee.Frangipani:Ascalable distributed file system. In Proc. 16th Symp. Operating Systems Principles (SOSP), pages 224–237, October 1997.
[29] R.vanRenesseandF.B.Schneider.Chainreplicationfor supporting high throughput and availability. In Proc. of the 6th OSDI, pages 91–104, 2004.
[30] H.YuandA.Vahdat.Designandevaluationofaconit-based continuous consistency model for replicated services. ACM Trans. Comput. Syst., 20(3):239–182, 2002.











